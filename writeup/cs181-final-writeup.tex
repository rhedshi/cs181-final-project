\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{commath}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% Declare commands
\newcommand{\mat}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{CS 181 -- Final Project}
\author{Casey Grun, Sam Kim, Rhed Shi}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

% -----------------------------------------------------------------------------
\section{Overview}

% -----------------------------------------------------------------------------
\section{Reinforcement Learning}



We pursued three independent approaches to solve this 
problem:

\subsection{Q-learning}

First, we implemented a the model-free ``Q-learning'' algorithm. Q-learning works
by learning the function $Q(s,a)$ (the expected value of an action $a$ from a 
state $s$). We begin with a uniform estimate of $Q(s,a) = 0 \forall s \forall a$.
At each time point during epoch $t$, we plan the next action $\pi(s)$ given the 
current state $s$, according to the following ``$\epsilon$-greedy'' policy:
$$\pi_t(s) = \begin{cases} 
\argmax_{a \in \mathcal{A}} Q(s,a) & \text{with probability } 1-\epsilon(t) \\
\text{random } a \in {0,1}         & \text{with probability } \epsilon(t) 
\end{cases}$$
where $\epsilon(t) = 1/t$. This allows exploration of new states at early time 
points and exploitation at later time points.

Upon taking an action $a$ (during epoch $t$), recieving a reward $r$, and observing a transition from $s$
to $s'$, we update our running estimate of $Q(s,a)$ as follows:
$$Q(s,a) \gets Q(s,a) + \alpha_k(s,a) \left[ (r + \gamma \max_{a' \in \mathcal{A}} Q(s', a')) - Q(s,a) \right].$$

We attempted a strategy where the learning rate $\alpha_k(s,a) = 1/k$, and $k$ is the number of times we've visited state $s$ and performed action $a$. This reflects our desire to gradually reduce the rate of change of our policy (and therefore $Q$) over epochs in order to exploit the benefits of our earlier exploration. However, we had much poorer results with this strategy than one where we simply set $\alpha_k(s,a) = 0.1 \forall(s,a,k)$. This could be a consequence of the decay rate being too slow or fast, because when we varied $\alpha$ as a function of iteration rather than $k$ such that $\alpha=0.01$ after 100 epochs and $\alpha=0.001$ after 200 epochs, we saw better results. When $\alpha$ was constant at 0.01 or even smaller, the score would steadily increase for the first couple hundred epochs, and then decrease back down to 0 after that, signifying that the model of $Q$ was diverging.

To improve learning rates, we modified the states to reduce the dimensionality of $Q$. First, the top and bottom of the tree and monkey as separate states are redundant because the monkey and the tree gap are constant heights. Next, rather than modelling the monkey and gap position separately, we used the difference in the monkey and gap position, ignoring the absolute position of either one. While at first this may seem to allow for the monkey hitting the top and bottom of the screen, we found this not to be teh case since the monkey would try to stay near the gap, which would never be off the screen. This change allowed our score to increase significantly.

Finally, we attempted to optimized the learning rate $\gamma$ and the number of bins into which each value was discretized. We determined each of these values by coordinate descent. The optimal value for $\gamma$ was about 0.4.  Appropriate bin sizes to balance convergence and performance within 500 epochs is around 20 bins for the height difference, and 10 bins for monkey velocity and distance from the tree.

In the end, we are able to consistently get scores of over 100, up to over 150, after a few hundred epochs. A representative time course over several epochs is shown below. Further improvement is certainly possible by optimizing each of the parameters (bins, $\alpha$, $\gamma$) more carefully and precisely, as we only estimated the optimal parameters.

% \includegraphics[width=\textwidth]{scores_model_free.pdf}

\subsection{Model-based learning}

Next, we implemented the model-based learning. In this model, we are not concerned with learning which actions are good or bad, but rather which states are good or bad and how the states are affected by our actions, and we circumvent the credit assignment problem. 

We keep track of the number of times a certain state $s$ is reached and action $a$ was taken in $N(s,a)$. Similarly, we keep track of the cumulative rewards for state $s$ and action $a$ in $R(s,a)$. Lastly, we keep track of the number of times state $s'$ was reached from state $s$ through action $a$ in $N(s,a,s')$. 

The probability of going from state $s$ to another state $s'$ is simply $N(s,a,s')/N(s,a)$, and we can generate the transition matrix by calculating the probabilities for each combination of states.

After every action, we update the policy using either value iteration or policy iteration as described above and below.

\subsection{TD-value learning}

Finally, we implemented temporal difference value learning. In this method, we learn a model of the value function $V(s)$, the transition model $P(s'|s,a)$, and the reward function $R(s,a)$. The transition model was learned as in the model-based approach above, where we maintained the empirical distribution. The reward function was learned by a running average, where each time we recieved a reward $r$ from taking action $a$ at state $s$ for the $k$-th time, we updated $R(s,a)$ as:
$$R(s,a) \gets (R(s,a) \cdot k + r)/(k + 1).$$

Each time we transitioned to state $s'$ from state $s$ and received reward $r$, we updated the value function according to:
$$V(s) \gets V(s) + \alpha ((r + \gamma V(s')) - V(s)).$$
At each step, we planned the next action $\pi(s)$ according to:
$$\pi(s) = \argmax_{a \in \mathcal{A}} \left[ R(s,a) + \sum_{s'} P(s'|s,a) V(s') \right]$$
where $R(s,a)$, $P(s'|s,a)$, and $V(s)$ represent our empirically-estimated values for those quantities.

We are able to get scores over 30 within 1000 iterations.

% -----------------------------------------------------------------------------
\section{Conclusion}


\end{document}